# =============================================================================
# FAAS4U â€” Laprophan | Couche Gold (Validation Engine)
# Fichier  : 03_Validation_Engine.py
# Auteure  : Salma | PFE Mundiapolis 2026
# RÃ´le     : Moteur de Backtesting PySpark
#             Calcul des 5 KPIs (MAPE, WAPE, Bias, Theil's U, Tracking Signal)
#             Ã‰valuation FVA (Human vs Algo) par Segment ABC/XYZ
# =============================================================================
# LOGIQUE MÃ‰TIER :
#   Compare les forecasts chargÃ©s (Silver) avec les rÃ©alisations IQVIA.
#   NE CRÃ‰E PAS de prÃ©visions, il se contente d'Ã©valuer celles qui existent.
#   GÃ©nÃ¨re des tables de rÃ©sultats (Gold) prÃªtes pour Power BI.
#
# RÃˆGLES ABSOLUES :
#   - PÃ©riode d'Ã©valuation dynamique 
#   - BPF/GMP : TracÃ© et versionnÃ©
#   - Calculs exacts selon formules mathÃ©matiques Laprophan
# =============================================================================

from pyspark.sql import SparkSession # type: ignore
from pyspark.sql import functions as F # type: ignore
from pyspark.sql.window import Window # type: ignore
import datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. SESSION SPARK & CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    spark = SparkSession.builder.getOrCreate()
except:
    pass

SILVER_TABLE = "Tables/silver/forecasts_clean"
GOLD_KPI_TABLE = "Tables/gold/forecast_kpis"
GOLD_FVA_TABLE = "Tables/gold/fva_results"
GOLD_ALERTS_TABLE = "Tables/gold/stock_alerts"

DATE_TRAITEMENT = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
AUDIT_USER = "Salma (Fabric Sync)"

def add_gold_audit(df, step_name):
    return df \
        .withColumn("sys_gold_load_date", F.lit(DATE_TRAITEMENT)) \
        .withColumn("sys_gold_step", F.lit(step_name))

print("\n" + "="*60)
print("Ã‰TAPE 1 â€” Chargement de la table Silver (forecasts_clean)")
print("="*60)

# Chargement de la table Silver gÃ©nÃ©rÃ©e Ã  l'Ã©tape prÃ©cÃ©dente
df_silver = spark.read.format("delta").load(SILVER_TABLE)

# On filtre uniquement sur les lignes oÃ¹ on a des ventes rÃ©elles (pour backtest)
df_eval = df_silver.filter(F.col("ventes_reelles_qty").isNotNull() & (F.col("ventes_reelles_qty") > 0))

print(f"Lignes Ã  Ã©valuer (avec Actuals) : {df_eval.count()}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. CALCUL DES KPIs DE BASE (Au niveau SKU / PÃ©riode / Algo)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*60)
print("Ã‰TAPE 2 â€” Calcul des mÃ©triques de base (Error, Abs_Error)")
print("="*60)

df_eval = df_eval.withColumn(
    "error_qty", 
    F.col("forecast_qty") - F.col("ventes_reelles_qty")
).withColumn(
    "abs_error_qty", 
    F.abs(F.col("error_qty"))
)

# Pour Theil's U : On a besoin du "NaÃ¯ve Forecast" (PrÃ©vision = Ventes rÃ©elles de la pÃ©riode prÃ©cÃ©dente)
window_naive = Window.partitionBy("sku_id").orderBy("periode")
df_eval = df_eval.withColumn(
    "naive_forecast", 
    F.lag("ventes_reelles_qty", 1).over(window_naive)
)
# Erreur du modÃ¨le naÃ¯f: (Actual_t - Actual_t-1)
df_eval = df_eval.withColumn(
    "naive_error_qty", 
    F.col("ventes_reelles_qty") - F.col("naive_forecast")
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. AGRÃ‰GATION DES KPIs GLOBAUX (Par SKU et Algo sur tout l'historique)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*60)
print("Ã‰TAPE 3 â€” AgrÃ©gation des KPIs (MAPE, WAPE, Bias, Theil's U)")
print("="*60)

df_kpi = df_eval.groupBy(
    "sku_id", "algorithme", "segment_abc", "segment_xyz", "segment_abcxyz", "mape_cible_pct"
).agg(
    # BIAIS (Moyen)
    F.mean("error_qty").alias("bias_moyen"),
    F.sum("error_qty").alias("sum_errors"),
    
    # MAE (Mean Absolute Error)
    F.mean("abs_error_qty").alias("mae"),
    F.sum("abs_error_qty").alias("sum_abs_errors"),
    
    # Ventes totales sur la pÃ©riode Ã©valuÃ©e
    F.sum("ventes_reelles_qty").alias("sum_actuals"),
    
    # Pre-calcul MAPE (moyenne des MAPEs individuels)
    F.mean(F.col("abs_error_qty") / F.col("ventes_reelles_qty")).alias("avg_mape_ratio"),
    
    # Pre-calcul Theil's U (Ratio des RMSE)
    F.sum(F.pow("error_qty", 2)).alias("sum_sq_errors"),
    F.sum(F.pow("naive_error_qty", 2)).alias("sum_sq_naive_errors")
)

# Finalisation des formules complexes
df_kpi = df_kpi.withColumn(
    "MAPE", F.round(F.col("avg_mape_ratio") * 100, 2)
).withColumn(
    "WAPE", 
    F.round(F.when(F.col("sum_actuals") > 0, (F.col("sum_abs_errors") / F.col("sum_actuals")) * 100).otherwise(None), 2)
).withColumn(
    # Tracking Signal = Sum of Errors / Mean Absolute Error (MAD)
    "Tracking_Signal", 
    F.round(F.when(F.col("mae") > 0, F.col("sum_errors") / F.col("mae")).otherwise(0), 2)
).withColumn(
    # Limited Theil's U = sqrt(sum(errors^2) / sum(naive_errors^2))
    # Si U < 1 : ModÃ¨le meilleur que prÃ©vision naÃ¯ve
    # Si U > 1 : ModÃ¨le pire que prÃ©vision naÃ¯ve
    "Theils_U",
    F.round(F.when(F.col("sum_sq_naive_errors") > 0, 
           F.sqrt(F.col("sum_sq_errors") / F.col("sum_sq_naive_errors"))).otherwise(None), 3)
)

# Classement global du meilleur Algo par SKU (BasÃ© sur le WAPE)
window_rank = Window.partitionBy("sku_id").orderBy("WAPE")
df_kpi = df_kpi.withColumn("Rank_WAPE", F.rank().over(window_rank))
df_kpi = df_kpi.withColumn("Is_Best_Algo", F.when(F.col("Rank_WAPE") == 1, True).otherwise(False))

# Audit BPF
df_kpi = add_gold_audit(df_kpi, "Calcul_KPI_Performance")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. CALCUL DU FVA (Forecast Value Added)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FVA = Mesure la valeur apportÃ©e par l'humain (Expert DP) vs le meilleur algorithme,
# ou vs le modÃ¨le NaÃ¯f.
# Formule : FVA = Erreur(Naive) - Erreur(Expert) [ou WAPE(Meilleur Algo) - WAPE(Expert)]
# Un FVA positif indique que l'expert a amÃ©liorÃ© la prÃ©vision.

print("\n" + "="*60)
print("Ã‰TAPE 4 â€” Calcul du FVA (Forecast Value Added)")
print("="*60)

# Isoler le WAPE de l'expert
df_expert = df_kpi.filter(F.col("algorithme") == "Expert_DP") \
                  .select("sku_id", F.col("WAPE").alias("WAPE_Expert"))

# Isoler le meilleur algorithme (hors expert)
df_best_algo = df_kpi.filter(F.col("algorithme") != "Expert_DP") \
                     .filter(F.col("Rank_WAPE") == 1) \
                     .select("sku_id", F.col("algorithme").alias("Best_Algo_Name"), 
                             F.col("WAPE").alias("WAPE_Best_Algo"))

df_fva = df_expert.join(df_best_algo, on="sku_id", how="inner")

# Calcul final du FVA
df_fva = df_fva.withColumn(
    "FVA_WAPE_Points", 
    F.round(F.col("WAPE_Best_Algo") - F.col("WAPE_Expert"), 2)
).withColumn(
    "Expert_Added_Value",
    F.when(F.col("FVA_WAPE_Points") > 0, "POSITIF (AmÃ©lioration)")
     .when(F.col("FVA_WAPE_Points") < 0, "NÃ‰GATIF (DÃ©gradation)")
     .otherwise("NEUTRE")
)

# On ramÃ¨ne la segmentation pour l'analyse FVA / Segment
df_fva = df_fva.join(df_kpi.select("sku_id", "segment_abcxyz").distinct(), on="sku_id", how="left")

df_fva = add_gold_audit(df_fva, "Calcul_FVA")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. GÃ‰NÃ‰RATION DES ALERTES (Tracking Signal Drift & Lead Time Impact)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*60)
print("Ã‰TAPE 5 â€” GÃ©nÃ©ration des alertes mÃ©tiers")
print("="*60)

# Alerte basÃ©e sur le Tracking Signal du meilleur Algorithme (+/- 4 est souvent la limite)
df_alerts = df_kpi.filter(F.col("Is_Best_Algo") == True) \
                  .select("sku_id", "segment_abcxyz", "algorithme", "Tracking_Signal", "bias_moyen", "mape_cible_pct", "MAPE")

# RÃ©cupÃ©ration du Lead Time depuis la table originale Silver
df_lt_info = df_silver.select("sku_id", "lt_total_jours").distinct()
df_alerts = df_alerts.join(df_lt_info, on="sku_id", how="left")

# Logique d'Alerte
df_alerts = df_alerts.withColumn(
    "Alert_Tracking_Signal",
    F.when(F.col("Tracking_Signal") > 4, "ALERTE SURSTOCK (Biais Positif Continu)")
     .when(F.col("Tracking_Signal") < -4, "ALERTE RUPTURE (Biais NÃ©gatif Continu)")
     .otherwise("OK")
).withColumn(
    "Alert_Cible_MAPE",
    F.when(F.col("MAPE") > F.col("mape_cible_pct"), "NON ATTEINTE")
     .otherwise("ATTEINTE")
)

df_alerts = add_gold_audit(df_alerts, "Generation_Alertes")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. SAUVEGARDE EN FORMAT DELTA (ONELAKE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*60)
print("Ã‰TAPE 6 â€” Ã‰criture des datamarts Gold")
print("="*60)

def save_gold(df, table_path):
    df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(table_path)
    print(f"âœ… SauvegardÃ© : {table_path} ({df.count()} lignes)")

save_gold(df_kpi, GOLD_KPI_TABLE)
save_gold(df_fva, GOLD_FVA_TABLE)
save_gold(df_alerts, GOLD_ALERTS_TABLE)

print("\nðŸŽ‰ MOTEUR DE BACKTESTING (GOLD) TERMINÃ‰ AVEC SUCCÃˆS.")
