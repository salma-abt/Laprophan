// =============================================================================
// FAAS4U â€” Laprophan | Power BI DAX Measures
// Fichier  : 04_DAX_Measures.dax
// Auteure  : Salma | PFE Mundiapolis 2026
// Encadrant: M. Kahlaoui
// RÃ´le     : Mesures DAX pour les scorecards Power BI
//             Sources : performance_metrics / fva_results / alertes_derive
// =============================================================================
// ORGANISATION :
//   Section 1  â€” Mesures de base (MAPE, WAPE, Biais, RMSE)
//   Section 2  â€” Tracking Signal et dÃ©rives
//   Section 3  â€” Limited Theil's U
//   Section 4  â€” Forecast Value Added (FVA)
//   Section 5  â€” Scorecards ABC/XYZ
//   Section 6  â€” KPIs Lead Time
//   Section 7  â€” Alertes et monitoring
//   Section 8  â€” Mesures de navigation et contexte
// =============================================================================


// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SECTION 1 â€” MESURES DE BASE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// MAPE moyen â€” filtrÃ© sur le contexte (segment, algo, horizon)
MAPE Moyen =
AVERAGEX(
    performance_metrics,
    performance_metrics[mape]
)

// MAPE pondÃ©rÃ© par le nombre d'observations
MAPE PondÃ©rÃ© =
DIVIDE(
    SUMX(performance_metrics, performance_metrics[mape] * performance_metrics[nb_observations]),
    SUM(performance_metrics[nb_observations]),
    BLANK()
)

// WAPE moyen
WAPE Moyen =
AVERAGEX(
    performance_metrics,
    performance_metrics[wape]
)

// MAE moyen (en unitÃ©s)
MAE Moyen =
AVERAGEX(
    performance_metrics,
    performance_metrics[mae]
)

// RMSE moyen
RMSE Moyen =
AVERAGEX(
    performance_metrics,
    performance_metrics[rmse]
)

// Biais moyen (positif = sur-prÃ©vision, nÃ©gatif = sous-prÃ©vision)
Biais Moyen =
AVERAGEX(
    performance_metrics,
    performance_metrics[biais_pct]
)

// Biais absolu moyen (amplitude sans direction)
Biais Absolu Moyen =
AVERAGEX(
    performance_metrics,
    ABS(performance_metrics[biais_pct])
)

// sMAPE moyen
sMAPE Moyen =
AVERAGEX(
    performance_metrics,
    performance_metrics[smape]
)

// Nombre total d'observations (SKU x pÃ©riode x algo)
Nb Observations =
SUM(performance_metrics[nb_observations])


// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SECTION 2 â€” TRACKING SIGNAL ET DÃ‰RIVES
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Tracking Signal moyen (seuil Â±6)
Tracking Signal Moyen =
AVERAGEX(
    performance_metrics,
    performance_metrics[tracking_signal]
)

// Valeur absolue du Tracking Signal (pour jauge)
Tracking Signal Absolu =
ABS([Tracking Signal Moyen])

// Nombre de dÃ©rives dÃ©tectÃ©es (Tracking Signal hors Â±6)
Nb DÃ©rives =
CALCULATE(
    COUNTROWS(performance_metrics),
    performance_metrics[flag_derive] = TRUE()
)

// % de combinaisons algo/segment en dÃ©rive
Taux DÃ©rive % =
DIVIDE(
    [Nb DÃ©rives],
    COUNTROWS(performance_metrics),
    0
) * 100

// Indicateur dÃ©rive â€” pour KPI card avec couleur conditionnelle
// Retourne "ALERTE" si dÃ©rive, "OK" sinon
Statut DÃ©rive =
IF(
    [Nb DÃ©rives] > 0,
    "âš  ALERTE â€” " & [Nb DÃ©rives] & " dÃ©rive(s) dÃ©tectÃ©e(s)",
    "âœ“ Stable"
)


// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SECTION 3 â€” LIMITED THEIL'S U
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Theil's U moyen
Theil U Moyen =
AVERAGEX(
    performance_metrics,
    performance_metrics[theil_u]
)

// % d'algorithmes avec valeur ajoutÃ©e rÃ©elle (U < 1)
% Algos Valeur AjoutÃ©e =
DIVIDE(
    CALCULATE(
        COUNTROWS(performance_metrics),
        performance_metrics[theil_u] < 1
    ),
    COUNTROWS(performance_metrics),
    0
) * 100

// InterprÃ©tation Theil's U â€” pour card visuelle
InterprÃ©tation Theil U =
VAR u = [Theil U Moyen]
RETURN
    SWITCH(
        TRUE(),
        u < 0.5,  "Excellent â€” bien meilleur que naÃ¯f",
        u < 1.0,  "Bon â€” meilleur que naÃ¯f",
        u = 1.0,  "Neutre â€” Ã©quivalent au naÃ¯f",
        u > 1.0,  "âš  Insuffisant â€” pire que naÃ¯f"
    )

// Theil's U du meilleur algorithme par segment (contexte filtrÃ©)
Theil U Meilleur Algo =
CALCULATE(
    MIN(performance_metrics[theil_u]),
    performance_metrics[theil_u] > 0
)


// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SECTION 4 â€” FORECAST VALUE ADDED (FVA)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// MAPE de la prÃ©vision naÃ¯ve (baseline)
MAPE NaÃ¯f =
AVERAGEX(
    fva_results,
    fva_results[mape_naive]
)

// MAPE du meilleur algorithme
MAPE Meilleur Algo =
AVERAGEX(
    fva_results,
    fva_results[mape_best_algo]
)

// MAPE aprÃ¨s correction Demand Planner
MAPE Demand Planner =
AVERAGEX(
    fva_results,
    fva_results[mape_dp]
)

// FVA de l'algorithme (gain vs baseline naÃ¯ve)
FVA Algo pts =
AVERAGEX(
    fva_results,
    fva_results[fva_algo_pts]
)

// FVA du Demand Planner (gain vs meilleur algo)
FVA Humain pts =
AVERAGEX(
    fva_results,
    fva_results[fva_humain_pts]
)

// FVA total (gain total vs baseline naÃ¯ve)
FVA Total pts =
AVERAGEX(
    fva_results,
    fva_results[fva_total_pts]
)

// Verdict global sur la valeur ajoutÃ©e humaine
Verdict FVA Humain =
VAR fva = [FVA Humain pts]
RETURN
    SWITCH(
        TRUE(),
        fva >  2,  "âœ“ Humain surperforme â€” conserver les corrections",
        fva >= 0,  "â†’ Humain neutre â€” Ã©valuer la pertinence",
        fva >= -2, "âš  Humain dÃ©grade lÃ©gÃ¨rement â€” former le DP",
        "âœ— Humain dÃ©grade significativement â€” bloquer les overrides"
    )

// % de segments oÃ¹ l'humain ajoute de la valeur
% Segments FVA Positif =
DIVIDE(
    CALCULATE(
        COUNTROWS(fva_results),
        fva_results[fva_humain_pts] > 0
    ),
    COUNTROWS(fva_results),
    0
) * 100

// Meilleur algorithme dans le contexte sÃ©lectionnÃ©
Meilleur Algorithme =
CALCULATE(
    FIRSTNONBLANK(fva_results[meilleur_algo], 1),
    TOPN(1, fva_results, fva_results[mape_best_algo], ASC)
)


// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SECTION 5 â€” SCORECARDS ABC/XYZ
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// MAPE par segment â€” utilisÃ© dans la heatmap ABC/XYZ
MAPE Segment =
CALCULATE(
    [MAPE Moyen],
    ALLEXCEPT(performance_metrics, performance_metrics[segment_abcxyz])
)

// Cible MAPE du segment sÃ©lectionnÃ©
Cible MAPE Segment =
CALCULATE(
    AVERAGE(performance_metrics[mape_cible]),
    ALLEXCEPT(performance_metrics, performance_metrics[segment_abc])
)

// Ã‰cart Ã  la cible MAPE (positif = hors cible)
Ã‰cart Cible MAPE =
[MAPE Moyen] - [Cible MAPE Segment]

// Couleur conditionnelle pour heatmap (retourne un code hex)
Couleur MAPE =
VAR ecart = [Ã‰cart Cible MAPE]
RETURN
    SWITCH(
        TRUE(),
        ecart <= -5,  "#1A7340",   // Vert foncÃ© â€” trÃ¨s en dessous de la cible
        ecart <= 0,   "#4CAF50",   // Vert â€” en dessous de la cible
        ecart <= 5,   "#FF9800",   // Orange â€” lÃ©gÃ¨rement au-dessus
        ecart <= 10,  "#F44336",   // Rouge â€” au-dessus
        "#8B0000"                  // Rouge foncÃ© â€” trÃ¨s au-dessus
    )

// % de SKUs segment A atteignant la cible MAPE < 10%
% SKUs A Cible Atteinte =
DIVIDE(
    CALCULATE(
        COUNTROWS(performance_metrics),
        performance_metrics[segment_abc] = "A",
        performance_metrics[mape] <= 10
    ),
    CALCULATE(
        COUNTROWS(performance_metrics),
        performance_metrics[segment_abc] = "A"
    ),
    0
) * 100

// % de SKUs segment B atteignant la cible MAPE < 20%
% SKUs B Cible Atteinte =
DIVIDE(
    CALCULATE(
        COUNTROWS(performance_metrics),
        performance_metrics[segment_abc] = "B",
        performance_metrics[mape] <= 20
    ),
    CALCULATE(
        COUNTROWS(performance_metrics),
        performance_metrics[segment_abc] = "B"
    ),
    0
) * 100

// % de SKUs segment C atteignant la cible MAPE < 40%
% SKUs C Cible Atteinte =
DIVIDE(
    CALCULATE(
        COUNTROWS(performance_metrics),
        performance_metrics[segment_abc] = "C",
        performance_metrics[mape] <= 40
    ),
    CALCULATE(
        COUNTROWS(performance_metrics),
        performance_metrics[segment_abc] = "C"
    ),
    0
) * 100

// Score de certification global (moyenne pondÃ©rÃ©e des 3 segments)
Score Certification Global =
( [% SKUs A Cible Atteinte] * 0.6 ) +
( [% SKUs B Cible Atteinte] * 0.3 ) +
( [% SKUs C Cible Atteinte] * 0.1 )

// Titre dynamique du scorecard (adaptÃ© au filtre sÃ©lectionnÃ©)
Titre Scorecard =
VAR seg = SELECTEDVALUE(performance_metrics[segment_abcxyz], "Tous segments")
VAR algo = SELECTEDVALUE(performance_metrics[algorithme], "Tous algorithmes")
VAR horizon = SELECTEDVALUE(performance_metrics[horizon_mois], 0)
RETURN
    "Performance " & algo &
    " | Segment " & seg &
    IF(horizon > 0, " | Horizon " & horizon & " mois", "")


// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SECTION 6 â€” KPIs LEAD TIME
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Ces mesures lisent la table forecasts_clean (Silver) qui contient
// les 4 Ã©tapes du Lead Time joinÃ©es avec les forecasts et actuals.

// Lead Time Besoins moyen (jours)
LT Besoins Moyen =
AVERAGE(forecasts_clean[lt_besoins_jours])

// Lead Time Fabrication moyen (jours)
LT Fabrication Moyen =
AVERAGE(forecasts_clean[lt_fabrication_jours])

// Lead Time LibÃ©ration moyen (jours)
LT LibÃ©ration Moyen =
AVERAGE(forecasts_clean[lt_liberation_jours])

// Lead Time Diffusion moyen (jours)
LT Diffusion Moyen =
AVERAGE(forecasts_clean[lt_diffusion_jours])

// Lead Time Total moyen (somme des 4 Ã©tapes)
LT Total Moyen =
AVERAGE(forecasts_clean[lt_total_jours])

// % de SKUs avec Lead Time Total > 90 jours (seuil critique)
% SKUs LT Critique =
DIVIDE(
    CALCULATE(
        DISTINCTCOUNT(forecasts_clean[sku_id]),
        forecasts_clean[lt_total_jours] > 90
    ),
    DISTINCTCOUNT(forecasts_clean[sku_id]),
    0
) * 100

// MAPE moyen sur les SKUs avec Lead Time anormal (> 90 jours)
// Permet de voir si le LT long dÃ©grade la prÃ©cision du forecast
MAPE SKUs LT Long =
CALCULATE(
    [MAPE Moyen],
    FILTER(
        forecasts_clean,
        forecasts_clean[lt_total_jours] > 90
    )
)

// CorrÃ©lation textuelle LT vs MAPE â€” pour card narrative
Impact LT sur MAPE =
VAR mape_lt_long  = [MAPE SKUs LT Long]
VAR mape_lt_court = CALCULATE(
    [MAPE Moyen],
    FILTER(forecasts_clean, forecasts_clean[lt_total_jours] <= 90)
)
VAR diff = mape_lt_long - mape_lt_court
RETURN
    SWITCH(
        TRUE(),
        diff > 10, "âš  Lead Time long dÃ©grade le MAPE de " & FORMAT(diff, "0.0") & " pts",
        diff > 5,  "â†’ Lead Time long impacte modÃ©rÃ©ment le MAPE (+" & FORMAT(diff, "0.0") & " pts)",
        diff > 0,  "Lead Time long : impact faible sur MAPE (+" & FORMAT(diff, "0.0") & " pts)",
        "âœ“ Lead Time sans impact significatif sur le MAPE"
    )

// % de jours en rupture (contexte Supply Chain)
Taux Rupture % =
DIVIDE(
    SUM(forecasts_clean[nb_ruptures]),
    COUNTROWS(forecasts_clean) * 30,
    0
) * 100

// MAPE sur les pÃ©riodes avec rupture vs sans rupture
MAPE avec Rupture =
CALCULATE(
    [MAPE Moyen],
    FILTER(forecasts_clean, forecasts_clean[flag_rupture] = TRUE())
)

MAPE sans Rupture =
CALCULATE(
    [MAPE Moyen],
    FILTER(forecasts_clean, forecasts_clean[flag_rupture] = FALSE())
)

// Impact des ruptures sur le MAPE â€” clÃ© pour distinguer Ã©cart algo vs opÃ©rationnel
Impact Rupture sur MAPE =
VAR diff = [MAPE avec Rupture] - [MAPE sans Rupture]
RETURN
    IF(
        diff > 5,
        "âš  Les ruptures gonflent le MAPE de " & FORMAT(diff, "0.0") & " pts â€” Ã©cart opÃ©rationnel",
        "âœ“ Les ruptures n'impactent pas significativement le MAPE"
    )


// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SECTION 7 â€” ALERTES ET MONITORING
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Nombre total d'alertes ouvertes
Nb Alertes Ouvertes =
CALCULATE(
    COUNTROWS(alertes_derive),
    alertes_derive[statut_alerte] = "OUVERTE"
)

// Nombre d'alertes critiques
Nb Alertes Critiques =
CALCULATE(
    COUNTROWS(alertes_derive),
    alertes_derive[criticite] = "CRITIQUE"
)

// Nombre d'alertes Ã©levÃ©es
Nb Alertes Ã‰levÃ©es =
CALCULATE(
    COUNTROWS(alertes_derive),
    alertes_derive[criticite] = "ELEVEE"
)

// Statut global du systÃ¨me â€” pour KPI card principale du dashboard
Statut Global SystÃ¨me =
VAR critiques = [Nb Alertes Critiques]
VAR Ã©levÃ©es   = [Nb Alertes Ã‰levÃ©es]
VAR total     = [Nb Alertes Ouvertes]
RETURN
    SWITCH(
        TRUE(),
        critiques > 0, "ðŸ”´ CRITIQUE â€” " & critiques & " alerte(s) critique(s)",
        Ã©levÃ©es   > 0, "ðŸŸ  ATTENTION â€” " & Ã©levÃ©es & " alerte(s) Ã©levÃ©e(s)",
        total     > 0, "ðŸŸ¡ SURVEILLANCE â€” " & total & " alerte(s) en cours",
        "ðŸŸ¢ NOMINAL â€” Aucune alerte"
    )

// % d'alertes Tracking Signal vs MAPE hors cible
% Alertes Tracking Signal =
DIVIDE(
    CALCULATE(
        COUNTROWS(alertes_derive),
        alertes_derive[type_alerte] = "TRACKING_SIGNAL"
    ),
    COUNTROWS(alertes_derive),
    0
) * 100

% Alertes MAPE Hors Cible =
DIVIDE(
    CALCULATE(
        COUNTROWS(alertes_derive),
        alertes_derive[type_alerte] = "MAPE_HORS_CIBLE"
    ),
    COUNTROWS(alertes_derive),
    0
) * 100

// DerniÃ¨re date de traitement (fraÃ®cheur des donnÃ©es)
DerniÃ¨re Mise Ã  Jour =
FORMAT(
    MAX(performance_metrics[date_traitement]),
    "DD/MM/YYYY HH:MM"
)

// Nombre de SKUs monitorÃ©s
Nb SKUs MonitorÃ©s =
DISTINCTCOUNT(forecasts_clean[sku_id])

// Nombre d'algorithmes Ã©valuÃ©s
Nb Algorithmes Ã‰valuÃ©s =
CALCULATE(
    DISTINCTCOUNT(performance_metrics[algorithme]),
    performance_metrics[algorithme] <> "Expert_DP"
)


// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SECTION 8 â€” MESURES DE NAVIGATION ET CONTEXTE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Horizon sÃ©lectionnÃ© â€” pour affichage dynamique
Horizon SÃ©lectionnÃ© =
VAR h = SELECTEDVALUE(performance_metrics[horizon_mois], -1)
RETURN
    IF(h = -1, "Tous horizons", h & " mois")

// Segment sÃ©lectionnÃ© â€” pour titre dynamique
Segment SÃ©lectionnÃ© =
SELECTEDVALUE(performance_metrics[segment_abcxyz], "Tous segments")

// Algorithme sÃ©lectionnÃ©
Algorithme SÃ©lectionnÃ© =
SELECTEDVALUE(performance_metrics[algorithme], "Tous algorithmes")

// Benchmark rapide â€” comparaison MAPE algo vs cible
Benchmark MAPE vs Cible =
VAR mape_actuel = [MAPE Moyen]
VAR cible       = [Cible MAPE Segment]
VAR statut      = IF(mape_actuel <= cible, "âœ“ Cible atteinte", "âœ— Hors cible")
VAR ecart       = ABS(mape_actuel - cible)
RETURN
    statut & " | MAPE = " & FORMAT(mape_actuel, "0.0") &
    "% | Cible = " & FORMAT(cible, "0.0") &
    "% | Ã‰cart = " & FORMAT(ecart, "0.0") & " pts"

// RÃ©sumÃ© exÃ©cutif â€” 1 ligne pour le header du dashboard
RÃ©sumÃ© ExÃ©cutif =
VAR nb_sku      = [Nb SKUs MonitorÃ©s]
VAR nb_algo     = [Nb Algorithmes Ã‰valuÃ©s]
VAR mape_global = FORMAT([MAPE Moyen], "0.0")
VAR nb_alertes  = [Nb Alertes Ouvertes]
VAR maj         = [DerniÃ¨re Mise Ã  Jour]
RETURN
    nb_sku & " SKUs | " &
    nb_algo & " algorithmes | " &
    "MAPE global = " & mape_global & "% | " &
    nb_alertes & " alerte(s) | Mis Ã  jour : " & maj

// =============================================================================
// FIN DES MESURES DAX
//
// TABLES UTILISÃ‰ES :
//   performance_metrics  â†’ Sections 1, 2, 3, 5, 7, 8
//   fva_results          â†’ Section 4
//   alertes_derive       â†’ Section 7
//   forecasts_clean      â†’ Section 6 (Lead Time + ruptures)
//
// VISUELS POWER BI RECOMMANDÃ‰S :
//   Page 1 â€” Scorecard principal
//     â†’ KPI cards : MAPE Moyen, Biais Moyen, Tracking Signal Absolu, Statut Global
//     â†’ Heatmap ABC/XYZ : Couleur MAPE par segment (matrice 3x3)
//     â†’ Graphique ligne : MAPE par horizon 1-24 mois par algorithme
//
//   Page 2 â€” FVA Humain vs Algorithmes
//     â†’ Graphique barres : mape_naive â†’ mape_best_algo â†’ mape_dp par segment
//     â†’ Table : Verdict FVA Humain par segment
//     â†’ KPI card : % Segments FVA Positif
//
//   Page 3 â€” Lead Time et OpÃ©rationnel
//     â†’ Barres empilÃ©es : LT Besoins + Fabrication + LibÃ©ration + Diffusion
//     â†’ Scatter : LT Total vs MAPE par SKU
//     â†’ Cards : Impact LT sur MAPE, Impact Rupture sur MAPE
//
//   Page 4 â€” Alertes et DÃ©rives
//     â†’ Table : alertes_derive avec criticitÃ©, recommandation Claude IA
//     â†’ KPI cards : Nb Alertes Critiques, Nb Alertes Ã‰levÃ©es
//     â†’ Graphique Tracking Signal dans le temps par algo/segment
// =============================================================================
